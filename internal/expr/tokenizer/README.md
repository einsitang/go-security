# Tokenizer

The tokenizer module is sourced from [einsitang/tokenizer](https://github.com/einsitang/tokenizer) (forked from bzick/tokenizer)

The modified code of the fork is incompatible with the main repository, so it is not maintained separately in the repository. Instead, it is integrated as a word segmentation tool for expressions.

if you need to know more about bzick/tokenizer project , link to [bzick/tokenizer](https://github.com/bzick/tokenizer)
